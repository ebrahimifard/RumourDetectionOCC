{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import spacy\n",
    "from textblob import TextBlob\n",
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "import pickle as pk\n",
    "import copy\n",
    "from sklearn import preprocessing\n",
    "import datetime\n",
    "import emoji\n",
    "from pprint import pprint\n",
    "from nltk.tree import Tree \n",
    "from stanfordcorenlp.corenlp import StanfordCoreNLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to Zubiaga path\n",
    "zubiagaPath = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to Kwon path\n",
    "kwonPath = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading zubiaga dataset into a multi-level dictionary\n",
    "zubiagaRumours = next(os.walk(zubiagaPath))[1]\n",
    "zubiagaTweets = {}\n",
    "zubiagaSourceTweets = {} \n",
    "try:\n",
    "    for topic in tqdm(zubiagaRumours):\n",
    "        zubiagaTweets[topic] = {\"rumour\":{}, \"nonRumour\":{}}\n",
    "        zubiagaSourceTweets[topic] = {\"rumour\":[], \"nonRumour\":[]}\n",
    "        for tweetCode in next(os.walk(zubiagaPath+topic+\"/rumours/\"))[1]:\n",
    "            zubiagaTweets[topic][\"rumour\"][tweetCode] = {\"source\":[] , \"reactions\":[]}\n",
    "            for tweet in os.listdir(zubiagaPath+topic+\"/rumours/\"+tweetCode+\"/source-tweets\"):\n",
    "                if tweet[0] != \".\":\n",
    "                    zubiagaTweets[topic][\"rumour\"][tweetCode][\"source\"] = json.load(open(zubiagaPath+topic+\"/rumours/\"+tweetCode+\"/source-tweets/\"+tweet,\"rb\"))\n",
    "                    zubiagaSourceTweets[topic][\"rumour\"].append(json.load(open(zubiagaPath+topic+\"/rumours/\"+tweetCode+\"/source-tweets/\"+tweet,\"rb\")))\n",
    "            for tweet in os.listdir(zubiagaPath+topic+\"/rumours/\"+tweetCode+\"/reactions\"):\n",
    "                if tweet[0] != \".\":\n",
    "                    zubiagaTweets[topic][\"rumour\"][tweetCode][\"reactions\"].append(json.load(open(zubiagaPath+topic+\"/rumours/\"+tweetCode+\"/reactions/\"+tweet,\"rb\")))\n",
    "        for tweetCode in next(os.walk(zubiagaPath+topic+\"/non-rumours/\"))[1]:\n",
    "            zubiagaTweets[topic][\"nonRumour\"][tweetCode] = {\"source\":[] , \"reactions\":[]}\n",
    "            for tweet in os.listdir(zubiagaPath+topic+\"/non-rumours/\"+tweetCode+\"/source-tweets\"):\n",
    "                if tweet[0] != \".\":\n",
    "                    zubiagaTweets[topic][\"nonRumour\"][tweetCode][\"source\"] = json.load(open(zubiagaPath+topic+\"/non-rumours/\"+tweetCode+\"/source-tweets/\"+tweet,\"rb\"))\n",
    "                    zubiagaSourceTweets[topic][\"nonRumour\"].append(json.load(open(zubiagaPath+topic+\"/non-rumours/\"+tweetCode+\"/source-tweets/\"+tweet,\"rb\")))\n",
    "            for tweet in os.listdir(zubiagaPath+topic+\"/non-rumours/\"+tweetCode+\"/reactions\"):\n",
    "                if tweet[0] != \".\":\n",
    "                    zubiagaTweets[topic][\"nonRumour\"][tweetCode][\"reactions\"].append(json.load(open(zubiagaPath+topic+\"/non-rumours/\"+tweetCode+\"/reactions/\"+tweet,\"rb\")))\n",
    "except:\n",
    "    print(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting the number of rumours and non-rumours\n",
    "r=0\n",
    "n=0\n",
    "for i in zubiagaTweets:\n",
    "    for j in zubiagaTweets[i]:\n",
    "        if j == \"rumour\":\n",
    "            r += len(zubiagaTweets[i][j])\n",
    "        elif j == \"nonRumour\":\n",
    "            n += len(zubiagaTweets[i][j])\n",
    "            \n",
    "#The correct number is rumour=2402 and non-rumour=4023\n",
    "print(f'r={r}, n={n}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Kwon dataset into a multi-level dictionary\n",
    "kwonRumours = next(os.walk(kwonPath))[1]\n",
    "kwonTweets = {}\n",
    "kwonSourceTweets = {} \n",
    "\n",
    "for topic in tqdm(kwonRumours):\n",
    "    if topic[0] == \"N\":\n",
    "        kwonSourceTweets[topic] = {\"nonRumour\":[]}    \n",
    "    elif topic[0] == \"R\":\n",
    "        kwonSourceTweets[topic] = {\"rumour\":[]} \n",
    "    for tweetCode in os.listdir(kwonPath+topic):\n",
    "        if topic[0] == \"N\":\n",
    "            kwonSourceTweets[topic][\"nonRumour\"].append(json.load(open(kwonPath+topic+\"/\"+tweetCode,\"rb\")))\n",
    "        elif topic[0] == \"R\":\n",
    "            kwonSourceTweets[topic][\"rumour\"].append(json.load(open(kwonPath+topic+\"/\"+tweetCode,\"rb\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting the number of rumour and non-rumour\n",
    "p,q = 0,0\n",
    "for top in kwonSourceTweets:\n",
    "    if top[0] == \"N\":\n",
    "        p += len(kwonSourceTweets[top][\"nonRumour\"])\n",
    "    elif top[0] == \"R\":\n",
    "        q += len(kwonSourceTweets[top][\"rumour\"])\n",
    "# The correct number is rumour=44394, nonRumour=96516\n",
    "print(f'rumour={q}, nonRumour={p}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing each tweet of Zubiaga dataset in a separate file for cognitive analysis (LIWC Scores)\n",
    "for topic in tqdm(zubiagaSourceTweets):\n",
    "    for category in zubiagaSourceTweets[topic]:\n",
    "        for tweet in zubiagaSourceTweets[topic][category]:\n",
    "            handle = open(f'./LIWC/Zubiaga/{topic}#{category}#{tweet[\"id\"]}.txt', encoding=\"utf-8\", mode=\"w\")\n",
    "            handle.write(tweet[\"text\"])\n",
    "            handle.close()\n",
    "\n",
    "# Writing each tweet of Kwon dataset in a separate file for cognitive analysis\n",
    "for topic in tqdm(kwonSourceTweets):\n",
    "    for category in kwonSourceTweets[topic]:\n",
    "        for tweet in kwonSourceTweets[topic][category]:\n",
    "            handle = open(f'./LIWC/Kwon/{topic}#{category}#{tweet[\"id\"]}.txt', encoding=\"utf-8\", mode=\"w\")\n",
    "            handle.write(tweet[\"full_text\"])\n",
    "            handle.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the results of LIWC for both KWON and Zubiaga dataset\n",
    "\n",
    "#Loading LIWC Roles\n",
    "kLIWCroles = [j.strip() for j in [i.replace(\"\\t\", \" \") for i in open(\"./LIWC/LIWC2015 Results (Kwon (140910 files)).txt\").readlines()][0].split(\" \")]\n",
    "zLIWCroles = [j.strip() for j in [i.replace(\"\\t\", \" \") for i in open(\"./LIWC/LIWC2015 Results (Zubiaga (6425 files)).txt\").readlines()][0].split(\" \")]\n",
    "\n",
    "#Loading LIWC Scores\n",
    "zLIWC_raw = [j.split(\" \") for j in [i.replace(\"\\t\", \" \").strip() for i in open(\"./LIWC/LIWC2015 Results (Zubiaga (6425 files)).txt\").readlines()][1:]]\n",
    "kLIWC_raw = [j.split(\" \") for j in [i.replace(\"\\t\", \" \").strip() for i in open(\"./LIWC/LIWC2015 Results (Kwon (140910 files)).txt\").readlines()][1:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a dictionary out of raw LIWC scores\n",
    "zLIWC = {}\n",
    "kLIWC = {}\n",
    "for i in zubiagaSourceTweets:\n",
    "    zLIWC[i]={\"rumour\":{}, \"nonRumour\":{}}\n",
    "for j in kwonSourceTweets:\n",
    "    kLIWC[j]={\"rumour\":{}, \"nonRumour\":{}}\n",
    "    \n",
    "\n",
    "for item in zLIWC_raw:\n",
    "    zipped = list(zip(zLIWCroles, item))\n",
    "    txt = zipped[0][1].replace(\".txt\",\"\").split(\"#\")\n",
    "    topic = txt[0]\n",
    "    category = txt[1]\n",
    "    _id = txt[2]\n",
    "    zLIWC[topic][category][_id]={}\n",
    "    for element in zipped:\n",
    "        zLIWC[topic][category][_id][element[0]] = element[1]\n",
    "\n",
    "for item in kLIWC_raw:\n",
    "    zipped = list(zip(kLIWCroles, item))\n",
    "    txt = zipped[0][1].replace(\".txt\",\"\").split(\"#\")\n",
    "    topic = txt[0]\n",
    "    category = txt[1]\n",
    "    _id = txt[2]\n",
    "    kLIWC[topic][category][_id]={}\n",
    "    for element in zipped:\n",
    "        kLIWC[topic][category][_id][element[0]] = element[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading abbreviations, vuglar terms and emoticons for feature extraction\n",
    "abbrAdr = \"./Abbreviations/abbr.txt\"\n",
    "abbrList = [w.strip() for w in open(abbrAdr).readlines() if w != \"\\n\"]\n",
    "    \n",
    "emotiAdr = \"./Emoticon/emoticons.txt\"\n",
    "emotiList = [w.strip() for w in open(emotiAdr).readlines() if w != \"\\n\"]\n",
    "\n",
    "vuglarAdr = \"./Vuglar terms/vuglarTerms.txt\"\n",
    "vuglarList = [w.strip() for w in open(vuglarAdr).readlines() if w != \"\\n\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature extraction from Zubiaga dataset\n",
    "\n",
    "nlp = spacy.load(\"en\")\n",
    "#Path to CoreNLP installation directory\n",
    "nlp2 = StanfordCoreNLP('')\n",
    "\n",
    "\n",
    "zubiagafeatures = {}\n",
    "for topic in tqdm(zubiagaSourceTweets):\n",
    "    zubiagafeatures[topic] = {}\n",
    "    for category in zubiagaSourceTweets[topic]:\n",
    "        zubiagafeatures[topic][category] = {}\n",
    "        for tweet in zubiagaSourceTweets[topic][category]:\n",
    "            zubiagafeatures[topic][category][tweet[\"id\"]] = {}\n",
    "            zubiagafeatures[topic][category][tweet[\"id\"]][\"category\"] = category\n",
    "            zubiagafeatures[topic][category][tweet[\"id\"]][\"topic\"] = topic\n",
    "            zubiagafeatures[topic][category][tweet[\"id\"]][\"id\"] = tweet[\"id\"]\n",
    "            zubiagafeatures[topic][category][tweet[\"id\"]][\"screenName\"] = tweet[\"user\"][\"screen_name\"]\n",
    "            zubiagafeatures[topic][category][tweet[\"id\"]][\"text\"] = tweet[\"text\"]\n",
    "            zubiagafeatures[topic][category][tweet[\"id\"]][\"tweetUrl\"] = \"https://twitter.com/\" + tweet[\"user\"][\"screen_name\"] + \"/status/\" + str(tweet[\"id\"]) \n",
    "            \n",
    "#Linguistic features\n",
    "\n",
    "            spacyTweetText = nlp(tweet[\"text\"])        \n",
    "            zubiagafeatures[topic][category][tweet[\"id\"]][\"exclamationMarkCount\"] = tweet[\"text\"].count(\"!\")\n",
    "            zubiagafeatures[topic][category][tweet[\"id\"]][\"questionMarkCount\"] = tweet[\"text\"].count(\"?\")\n",
    "            zubiagafeatures[topic][category][tweet[\"id\"]][\"characterCount\"] = len(tweet[\"text\"])\n",
    "            zubiagafeatures[topic][category][tweet[\"id\"]][\"tokenCount\"] = len(spacyTweetText)\n",
    "            zubiagafeatures[topic][category][tweet[\"id\"]][\"subjectivity\"] = TextBlob(tweet[\"text\"]).sentiment.subjectivity\n",
    "            zubiagafeatures[topic][category][tweet[\"id\"]][\"polarity\"] = TextBlob(tweet[\"text\"]).sentiment.polarity\n",
    "            zubiagafeatures[topic][category][tweet[\"id\"]][\"uppercaseCount\"] = sum(1 for i in tweet[\"text\"] if i.isupper())\n",
    "            zubiagafeatures[topic][category][tweet[\"id\"]][\"lowerCaseCount\"] = sum(1 for i in tweet[\"text\"] if i.islower())\n",
    "            zubiagafeatures[topic][category][tweet[\"id\"]][\"firstPersonPronounCount\"] = tweet[\"text\"].lower().split(\" \").count(\"i\") + tweet[\"text\"].lower().split(\" \").count(\"me\") + tweet[\"text\"].lower().split(\" \").count(\"my\") + tweet[\"text\"].lower().split(\" \").count(\"mine\") + tweet[\"text\"].lower().split(\" \").count(\"we\") + tweet[\"text\"].lower().split(\" \").count(\"us\") + tweet[\"text\"].lower().split(\" \").count(\"our\")  + tweet[\"text\"].lower().split(\" \").count(\"ours\") + tweet[\"text\"].lower().split(\" \").count(\"i'm\") + tweet[\"text\"].lower().split(\" \").count(\"we're\") + tweet[\"text\"].lower().split(\" \").count(\"i've\") + tweet[\"text\"].lower().split(\" \").count(\"we've\") + tweet[\"text\"].lower().split(\" \").count(\"i'd\") + tweet[\"text\"].lower().split(\" \").count(\"we'd\")\n",
    "            zubiagafeatures[topic][category][tweet[\"id\"]][\"secondPersonPronounCount\"] = tweet[\"text\"].lower().split(\" \").count(\"you\") + tweet[\"text\"].lower().split(\" \").count(\"your\") + tweet[\"text\"].lower().split(\" \").count(\"yours\") +  tweet[\"text\"].lower().split(\" \").count(\"you're\") + tweet[\"text\"].lower().split(\" \").count(\"you've\") + tweet[\"text\"].lower().split(\" \").count(\"you'd\")\n",
    "            zubiagafeatures[topic][category][tweet[\"id\"]][\"thirdPersonPronounCount\"] = tweet[\"text\"].lower().split(\" \").count(\"he\") + tweet[\"text\"].lower().split(\" \").count(\"she\") + tweet[\"text\"].lower().split(\" \").count(\"it\") + tweet[\"text\"].lower().split(\" \").count(\"his\") + tweet[\"text\"].lower().split(\" \").count(\"her\") + tweet[\"text\"].lower().split(\" \").count(\"its\") + tweet[\"text\"].lower().split(\" \").count(\"him\") + tweet[\"text\"].lower().split(\" \").count(\"hers\") + tweet[\"text\"].lower().split(\" \").count(\"they\") + tweet[\"text\"].lower().split(\" \").count(\"them\") + tweet[\"text\"].lower().split(\" \").count(\"their\") + tweet[\"text\"].lower().split(\" \").count(\"theirs\")+ tweet[\"text\"].lower().split(\" \").count(\"they're\") + tweet[\"text\"].lower().split(\" \").count(\"he's\") + tweet[\"text\"].lower().split(\" \").count(\"she's\") + tweet[\"text\"].lower().split(\" \").count(\"it's\") + tweet[\"text\"].lower().split(\" \").count(\"they've\") + tweet[\"text\"].lower().split(\" \").count(\"they'd\") + tweet[\"text\"].lower().split(\" \").count(\"he'd\") + tweet[\"text\"].lower().split(\" \").count(\"she'd\") + tweet[\"text\"].lower().split(\" \").count(\"it'd\")\n",
    "            zubiagafeatures[topic][category][tweet[\"id\"]][\"capitalWordsCount\"] = len([b for b in [i for i in tweet[\"text\"].split(\" \")] if b.isupper()])\n",
    "            zubiagafeatures[topic][category][tweet[\"id\"]][\"averageWordComplexity\"] = np.average([len(i) for i in tweet[\"text\"].split(\" \")])\n",
    "            zubiagafeatures[topic][category][tweet[\"id\"]][\"vuglarTermsCount\"] = len([a for a in tweet[\"text\"].split(\" \") if a.lower() in vuglarList])\n",
    "            zubiagafeatures[topic][category][tweet[\"id\"]][\"emoticonCount\"] = len([a for a in tweet[\"text\"].split(\" \") if a.lower() in emotiList])\n",
    "            zubiagafeatures[topic][category][tweet[\"id\"]][\"abbreviationCount\"] = len([a for a in tweet[\"text\"].split(\" \") if a.lower() in abbrList])\n",
    "            zubiagafeatures[topic][category][tweet[\"id\"]][\"emojiCount\"] = len([x for x in tweet[\"text\"].split(\" \") if x in emoji.EMOJI_UNICODE.keys() or x in emoji.EMOJI_UNICODE.values()])\n",
    "            \n",
    "            zubiagafeatures[topic][category][tweet[\"id\"]][\"posAdjectiveCount\"] = len([x for x in spacyTweetText if x.pos_ == \"ADJ\"])\n",
    "            zubiagafeatures[topic][category][tweet[\"id\"]][\"posAdpositionCount\"] = len([x for x in spacyTweetText if x.pos_ == \"ADP\"])\n",
    "            zubiagafeatures[topic][category][tweet[\"id\"]][\"posAdverbCount\"] = len([x for x in spacyTweetText if x.pos_ == \"ADV\"])\n",
    "            zubiagafeatures[topic][category][tweet[\"id\"]][\"posAuxiliaryCount\"] = len([x for x in spacyTweetText if x.pos_ == \"AUX\"])\n",
    "            zubiagafeatures[topic][category][tweet[\"id\"]][\"posConjunctionCount\"] = len([x for x in spacyTweetText if x.pos_ == \"CONJ\"])\n",
    "            zubiagafeatures[topic][category][tweet[\"id\"]][\"posCoordinatingConjunctionCount\"] = len([x for x in spacyTweetText if x.pos_ == \"CCONJ\"])\n",
    "            zubiagafeatures[topic][category][tweet[\"id\"]][\"posDeterminerCount\"] = len([x for x in spacyTweetText if x.pos_ == \"DET\"])\n",
    "            zubiagafeatures[topic][category][tweet[\"id\"]][\"posInterjectionCount\"] = len([x for x in spacyTweetText if x.pos_ == \"INTJ\"])\n",
    "            zubiagafeatures[topic][category][tweet[\"id\"]][\"posNounCount\"] = len([x for x in spacyTweetText if x.pos_ == \"NOUN\"])\n",
    "            zubiagafeatures[topic][category][tweet[\"id\"]][\"posNumeralCount\"] = len([x for x in spacyTweetText if x.pos_ == \"NUM\"])\n",
    "            zubiagafeatures[topic][category][tweet[\"id\"]][\"posParticleCount\"] = len([x for x in spacyTweetText if x.pos_ == \"PART\"])\n",
    "            zubiagafeatures[topic][category][tweet[\"id\"]][\"posPronounCount\"] = len([x for x in spacyTweetText if x.pos_ == \"PRON\"])\n",
    "            zubiagafeatures[topic][category][tweet[\"id\"]][\"posProperNounCount\"] = len([x for x in spacyTweetText if x.pos_ == \"PROPN\"])\n",
    "            zubiagafeatures[topic][category][tweet[\"id\"]][\"posPunctuationCount\"] = len([x for x in spacyTweetText if x.pos_ == \"PUNCT\"])\n",
    "            zubiagafeatures[topic][category][tweet[\"id\"]][\"posSubordinatingConjunctionCount\"] = len([x for x in spacyTweetText if x.pos_ == \"SCONJ\"])\n",
    "            zubiagafeatures[topic][category][tweet[\"id\"]][\"posSymbolCount\"] = len([x for x in spacyTweetText if x.pos_ == \"SYM\"])\n",
    "            zubiagafeatures[topic][category][tweet[\"id\"]][\"posVerbCount\"] = len([x for x in spacyTweetText if x.pos_ == \"VERB\"])\n",
    "            zubiagafeatures[topic][category][tweet[\"id\"]][\"posOtherCount\"] = len([x for x in spacyTweetText if x.pos_ == \"X\"])\n",
    "            zubiagafeatures[topic][category][tweet[\"id\"]][\"posSpaceCount\"] = len([x for x in spacyTweetText if x.pos_ == \"SPACE\"])\n",
    "            zubiagafeatures[topic][category][tweet[\"id\"]][\"nerPersonCount\"] = len([x for x in spacyTweetText if x.ent_type_ == \"Person\"])\n",
    "            zubiagafeatures[topic][category][tweet[\"id\"]][\"nerNationalityCount\"] = len([x for x in spacyTweetText if x.ent_type_ == \"NORP\"])\n",
    "            zubiagafeatures[topic][category][tweet[\"id\"]][\"nerBuildingCount\"] = len([x for x in spacyTweetText if x.ent_type_ == \"FAC\"])\n",
    "            zubiagafeatures[topic][category][tweet[\"id\"]][\"nerOrganizationCount\"] = len([x for x in spacyTweetText if x.ent_type_ == \"ORG\"])\n",
    "            zubiagafeatures[topic][category][tweet[\"id\"]][\"nerCountriesCount\"] = len([x for x in spacyTweetText if x.ent_type_ == \"GPE\"])\n",
    "            zubiagafeatures[topic][category][tweet[\"id\"]][\"nerLocationCount\"] = len([x for x in spacyTweetText if x.ent_type_ == \"LOC\"])\n",
    "            zubiagafeatures[topic][category][tweet[\"id\"]][\"nerProductCount\"] = len([x for x in spacyTweetText if x.ent_type_ == \"PRODUCT\"])\n",
    "            zubiagafeatures[topic][category][tweet[\"id\"]][\"nerEventCount\"] = len([x for x in spacyTweetText if x.ent_type_ == \"EVENT\"])\n",
    "            zubiagafeatures[topic][category][tweet[\"id\"]][\"nerArtCount\"] = len([x for x in spacyTweetText if x.ent_type_ == \"WORK_OF_ART\"])\n",
    "            zubiagafeatures[topic][category][tweet[\"id\"]][\"nerLawCount\"] = len([x for x in spacyTweetText if x.ent_type_ == \"LAW\"])\n",
    "            zubiagafeatures[topic][category][tweet[\"id\"]][\"nerLanguageCount\"] = len([x for x in spacyTweetText if x.ent_type_ == \"LANGUAGE\"])\n",
    "            zubiagafeatures[topic][category][tweet[\"id\"]][\"nerDateCount\"] = len([x for x in spacyTweetText if x.ent_type_ == \"DATE\"])\n",
    "            zubiagafeatures[topic][category][tweet[\"id\"]][\"nerTimeCount\"] = len([x for x in spacyTweetText if x.ent_type_ == \"TIME\"])\n",
    "            zubiagafeatures[topic][category][tweet[\"id\"]][\"nerMoneyCount\"] = len([x for x in spacyTweetText if x.ent_type_ == \"MONEY\"])\n",
    "            zubiagafeatures[topic][category][tweet[\"id\"]][\"nerQuantityCount\"] = len([x for x in spacyTweetText if x.ent_type_ == \"QUANTITY\"])\n",
    "            zubiagafeatures[topic][category][tweet[\"id\"]][\"nerOrdinalCount\"] = len([x for x in spacyTweetText if x.ent_type_ == \"ORDINAL\"])\n",
    "            zubiagafeatures[topic][category][tweet[\"id\"]][\"nerCardinalCount\"] = len([x for x in spacyTweetText if x.ent_type_ == \"Cardinal\"])  \n",
    "\n",
    "            zubiagafeatures[topic][category][tweet[\"id\"]][\"insight\"] = zLIWC[topic][category][str(tweet[\"id\"])][\"insight\"]\n",
    "            zubiagafeatures[topic][category][tweet[\"id\"]][\"tentative\"] = zLIWC[topic][category][str(tweet[\"id\"])][\"tentat\"]\n",
    "            zubiagafeatures[topic][category][tweet[\"id\"]][\"positiveEmotion\"] = zLIWC[topic][category][str(tweet[\"id\"])][\"posemo\"]\n",
    "            zubiagafeatures[topic][category][tweet[\"id\"]][\"negativeEmotion\"] = zLIWC[topic][category][str(tweet[\"id\"])][\"negemo\"]\n",
    "            zubiagafeatures[topic][category][tweet[\"id\"]][\"anxiety\"] = zLIWC[topic][category][str(tweet[\"id\"])][\"anx\"]\n",
    "            zubiagafeatures[topic][category][tweet[\"id\"]][\"certainty\"] = zLIWC[topic][category][str(tweet[\"id\"])][\"certain\"]\n",
    "            zubiagafeatures[topic][category][tweet[\"id\"]][\"tone\"] = zLIWC[topic][category][str(tweet[\"id\"])][\"Tone\"]\n",
    "            parser=nlp2.parse(tweet[\"text\"]) \n",
    "            tree=Tree.fromstring(parser.__str__()) \n",
    "            zubiagafeatures[topic][category][tweet[\"id\"]][\"sentenceComplexity\"] = tree.height() \n",
    "         \n",
    "# User features\n",
    "\n",
    "            zubiagafeatures[topic][category][tweet[\"id\"]][\"hasProfileDescription\"] = True if tweet[\"user\"][\"description\"] != '' else False\n",
    "            zubiagafeatures[topic][category][tweet[\"id\"]][\"isVerifiedAccount\"] = tweet[\"user\"][\"verified\"] \n",
    "            zubiagafeatures[topic][category][tweet[\"id\"]][\"statusCount\"] = tweet[\"user\"][\"statuses_count\"]\n",
    "            zubiagafeatures[topic][category][tweet[\"id\"]][\"followingCount\"] = tweet[\"user\"][\"friends_count\"]\n",
    "            zubiagafeatures[topic][category][tweet[\"id\"]][\"influnece\"] = tweet[\"user\"][\"followers_count\"]\n",
    "            try:\n",
    "                zubiagafeatures[topic][category][tweet[\"id\"]][\"userRole\"] = tweet[\"user\"][\"followers_count\"]/tweet[\"user\"][\"friends_count\"]\n",
    "            except:\n",
    "                zubiagafeatures[topic][category][tweet[\"id\"]][\"userRole\"] = float(\"inf\")\n",
    "            zubiagafeatures[topic][category][tweet[\"id\"]][\"totalProfileLikesCount\"] = tweet[\"user\"][\"favourites_count\"]\n",
    "            accountCreationTime = datetime.datetime.strptime(tweet[\"user\"][\"created_at\"], \"%a %b %d %H:%M:%S %z %Y\")\n",
    "            today = datetime.datetime.now()\n",
    "            zubiagafeatures[topic][category][tweet[\"id\"]][\"accountAge\"] = (today.date() - accountCreationTime.date()).days\n",
    "            zubiagafeatures[topic][category][tweet[\"id\"]][\"protectedProfile\"] = tweet[\"user\"][\"protected\"]\n",
    "            zubiagafeatures[topic][category][tweet[\"id\"]][\"hasProfileLocation\"] = True if tweet[\"user\"][\"location\"] != '' else False\n",
    "            zubiagafeatures[topic][category][tweet[\"id\"]][\"profileLocation\"] = tweet[\"user\"][\"location\"]\n",
    "            zubiagafeatures[topic][category][tweet[\"id\"]][\"hasProfilePicture\"] = True if tweet[\"user\"][\"profile_image_url\"] != '' else False\n",
    "            zubiagafeatures[topic][category][tweet[\"id\"]][\"geoEnabled\"] = tweet[\"user\"][\"geo_enabled\"]\n",
    "            zubiagafeatures[topic][category][tweet[\"id\"]][\"hasProfileUrl\"] = tweet[\"user\"][\"url\"]\n",
    "            zubiagafeatures[topic][category][tweet[\"id\"]][\"averageFollowSpeed\"] = tweet[\"user\"][\"followers_count\"] / zubiagafeatures[topic][category][tweet[\"id\"]][\"accountAge\"]\n",
    "            zubiagafeatures[topic][category][tweet[\"id\"]][\"averageBeingFollowedSpeed\"] = tweet[\"user\"][\"friends_count\"] / zubiagafeatures[topic][category][tweet[\"id\"]][\"accountAge\"]\n",
    "            zubiagafeatures[topic][category][tweet[\"id\"]][\"averageLikeSpeed\"] = tweet[\"user\"][\"favourites_count\"] / zubiagafeatures[topic][category][tweet[\"id\"]][\"accountAge\"]\n",
    "            zubiagafeatures[topic][category][tweet[\"id\"]][\"averageStatusSpeed\"] = tweet[\"user\"][\"statuses_count\"] / zubiagafeatures[topic][category][tweet[\"id\"]][\"accountAge\"]\n",
    "            zubiagafeatures[topic][category][tweet[\"id\"]][\"screenNameLength\"] = len(tweet[\"user\"][\"screen_name\"])\n",
    "            zubiagafeatures[topic][category][tweet[\"id\"]][\"screenNameDigitCount\"] = len([i for i in tweet[\"user\"][\"screen_name\"] if i in [str(k) for k in range(0,10)]])\n",
    "            \n",
    "# Meta message features\n",
    "        \n",
    "            zubiagafeatures[topic][category][tweet[\"id\"]][\"hashtagCount\"] = len(tweet[\"entities\"][\"hashtags\"])\n",
    "            zubiagafeatures[topic][category][tweet[\"id\"]][\"mentionCount\"] = len(tweet[\"entities\"][\"user_mentions\"])\n",
    "            zubiagafeatures[topic][category][tweet[\"id\"]][\"hasUrl\"] = (\"http://\" in tweet[\"text\"] or \"https://\" in tweet[\"text\"] or len(tweet[\"entities\"][\"urls\"]) > 0)\n",
    "            zubiagafeatures[topic][category][tweet[\"id\"]][\"multimediaCounter\"] = len(tweet[\"entities\"][\"media\"]) if \"media\" in tweet[\"entities\"].keys() else 0\n",
    "            zubiagafeatures[topic][category][tweet[\"id\"]][\"likeCount\"] = tweet[\"favorite_count\"]\n",
    "            zubiagafeatures[topic][category][tweet[\"id\"]][\"retweetCount\"] = tweet[\"retweet_count\"]\n",
    "            tweetCreationTime = datetime.datetime.strptime(tweet[\"created_at\"], \"%a %b %d %H:%M:%S %z %Y\")\n",
    "            zubiagafeatures[topic][category][tweet[\"id\"]][\"tweetPostTime\"] = tweetCreationTime.hour*3600 + tweetCreationTime.minute*60 + tweetCreationTime.second \n",
    "            zubiagafeatures[topic][category][tweet[\"id\"]][\"place\"] = tweet[\"place\"]\n",
    "            \n",
    "#Compound score\n",
    "\n",
    "            zubiagafeatures[topic][category][tweet[\"id\"]][\"credibilityScore\"] = tweet[\"user\"][\"verified\"] \\\n",
    "                                                                                * (zubiagafeatures[topic][category][tweet[\"id\"]][\"accountAge\"] + tweet[\"user\"][\"followers_count\"] \\\n",
    "                                                                                   + tweet[\"user\"][\"statuses_count\"] + tweet[\"favorite_count\"]) \n",
    "            zubiagafeatures[topic][category][tweet[\"id\"]][\"engagementScore\"] = (tweet[\"user\"][\"statuses_count\"] + tweet[\"favorite_count\"])/zubiagafeatures[topic][category][tweet[\"id\"]][\"accountAge\"]\n",
    "\n",
    "#Other features\n",
    "            \n",
    "            pattern = re.compile('[>].*[<]')\n",
    "            try:\n",
    "                zubiagafeatures[topic][category][tweet[\"id\"]][\"source\"] = pattern.findall(tweet[\"source\"])[0][1:-1]\n",
    "            except:\n",
    "                zubiagafeatures[topic][category][tweet[\"id\"]][\"source\"] = tweet[\"source\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature extraction from Kwon dataset\n",
    "\n",
    "nlp = spacy.load(\"en\")\n",
    "nlp2 = StanfordCoreNLP('D:/Program Files (x86)/StanfordCoreNLP/stanford-corenlp-full-2018-02-27')\n",
    "excepts=[]\n",
    "\n",
    "kwonfeatures = {}\n",
    "for topic in tqdm(kwonSourceTweets):\n",
    "    kwonfeatures[topic] = {}\n",
    "    for category in kwonSourceTweets[topic]:\n",
    "        kwonfeatures[topic][category] = {}\n",
    "        for tweet in kwonSourceTweets[topic][category]:\n",
    "            kwonfeatures[topic][category][tweet[\"id\"]] = {}\n",
    "            kwonfeatures[topic][category][tweet[\"id\"]][\"category\"] = category\n",
    "            kwonfeatures[topic][category][tweet[\"id\"]][\"topic\"] = topic\n",
    "            kwonfeatures[topic][category][tweet[\"id\"]][\"id\"] = tweet[\"id\"]\n",
    "            kwonfeatures[topic][category][tweet[\"id\"]][\"screenName\"] = tweet[\"user\"][\"screen_name\"]\n",
    "            kwonfeatures[topic][category][tweet[\"id\"]][\"text\"] = tweet[\"full_text\"]\n",
    "            kwonfeatures[topic][category][tweet[\"id\"]][\"tweetUrl\"] = \"https://twitter.com/\" + tweet[\"user\"][\"screen_name\"] + \"/status/\" + str(tweet[\"id\"]) \n",
    "            \n",
    "#Linguistic features\n",
    "\n",
    "            spacyTweetText = nlp(tweet[\"full_text\"])\n",
    "            kwonfeatures[topic][category][tweet[\"id\"]][\"exclamationMarkCount\"] = tweet[\"full_text\"].count(\"!\")\n",
    "            kwonfeatures[topic][category][tweet[\"id\"]][\"questionMarkCount\"] = tweet[\"full_text\"].count(\"?\")\n",
    "            kwonfeatures[topic][category][tweet[\"id\"]][\"characterCount\"] = len(tweet[\"full_text\"])\n",
    "            kwonfeatures[topic][category][tweet[\"id\"]][\"tokenCount\"] = len(spacyTweetText)\n",
    "            kwonfeatures[topic][category][tweet[\"id\"]][\"subjectivity\"] = TextBlob(tweet[\"full_text\"]).sentiment.subjectivity\n",
    "            kwonfeatures[topic][category][tweet[\"id\"]][\"polarity\"] = TextBlob(tweet[\"full_text\"]).sentiment.polarity\n",
    "            kwonfeatures[topic][category][tweet[\"id\"]][\"uppercaseCount\"] = sum(1 for i in tweet[\"full_text\"] if i.isupper())\n",
    "            kwonfeatures[topic][category][tweet[\"id\"]][\"lowerCaseCount\"] = sum(1 for i in tweet[\"full_text\"] if i.islower())\n",
    "            kwonfeatures[topic][category][tweet[\"id\"]][\"firstPersonPronounCount\"] = tweet[\"full_text\"].lower().split(\" \").count(\"i\") + tweet[\"full_text\"].lower().split(\" \").count(\"me\") + tweet[\"full_text\"].lower().split(\" \").count(\"my\") + tweet[\"full_text\"].lower().split(\" \").count(\"mine\") + tweet[\"full_text\"].lower().split(\" \").count(\"we\") + tweet[\"full_text\"].lower().split(\" \").count(\"us\") + tweet[\"full_text\"].lower().split(\" \").count(\"our\")  + tweet[\"full_text\"].lower().split(\" \").count(\"ours\") + tweet[\"full_text\"].lower().split(\" \").count(\"i'm\") + tweet[\"full_text\"].lower().split(\" \").count(\"we're\") + tweet[\"full_text\"].lower().split(\" \").count(\"i've\") + tweet[\"full_text\"].lower().split(\" \").count(\"we've\") + tweet[\"full_text\"].lower().split(\" \").count(\"i'd\") + tweet[\"full_text\"].lower().split(\" \").count(\"we'd\")\n",
    "            kwonfeatures[topic][category][tweet[\"id\"]][\"secondPersonPronounCount\"] = tweet[\"full_text\"].lower().split(\" \").count(\"you\") + tweet[\"full_text\"].lower().split(\" \").count(\"your\") + tweet[\"full_text\"].lower().split(\" \").count(\"yours\") +  tweet[\"full_text\"].lower().split(\" \").count(\"you're\") + tweet[\"full_text\"].lower().split(\" \").count(\"you've\") + tweet[\"full_text\"].lower().split(\" \").count(\"you'd\")\n",
    "            kwonfeatures[topic][category][tweet[\"id\"]][\"thirdPersonPronounCount\"] = tweet[\"full_text\"].lower().split(\" \").count(\"he\") + tweet[\"full_text\"].lower().split(\" \").count(\"she\") + tweet[\"full_text\"].lower().split(\" \").count(\"it\") + tweet[\"full_text\"].lower().split(\" \").count(\"his\") + tweet[\"full_text\"].lower().split(\" \").count(\"her\") + tweet[\"full_text\"].lower().split(\" \").count(\"its\") + tweet[\"full_text\"].lower().split(\" \").count(\"him\") + tweet[\"full_text\"].lower().split(\" \").count(\"hers\") + tweet[\"full_text\"].lower().split(\" \").count(\"they\") + tweet[\"full_text\"].lower().split(\" \").count(\"them\") + tweet[\"full_text\"].lower().split(\" \").count(\"their\") + tweet[\"full_text\"].lower().split(\" \").count(\"theirs\")+ tweet[\"full_text\"].lower().split(\" \").count(\"they're\") + tweet[\"full_text\"].lower().split(\" \").count(\"he's\") + tweet[\"full_text\"].lower().split(\" \").count(\"she's\") + tweet[\"full_text\"].lower().split(\" \").count(\"it's\") + tweet[\"full_text\"].lower().split(\" \").count(\"they've\") + tweet[\"full_text\"].lower().split(\" \").count(\"they'd\") + tweet[\"full_text\"].lower().split(\" \").count(\"he'd\") + tweet[\"full_text\"].lower().split(\" \").count(\"she'd\") + tweet[\"full_text\"].lower().split(\" \").count(\"it'd\")\n",
    "            kwonfeatures[topic][category][tweet[\"id\"]][\"capitalWordsCount\"] = len([b for b in [i for i in tweet[\"full_text\"].split(\" \")] if b.isupper()])\n",
    "            kwonfeatures[topic][category][tweet[\"id\"]][\"averageWordComplexity\"] = np.average([len(i) for i in tweet[\"full_text\"].split(\" \")])\n",
    "            kwonfeatures[topic][category][tweet[\"id\"]][\"vuglarTermsCount\"] = len([a for a in tweet[\"full_text\"].split(\" \") if a.lower() in vuglarList])\n",
    "            kwonfeatures[topic][category][tweet[\"id\"]][\"emoticonCount\"] = len([a for a in tweet[\"full_text\"].split(\" \") if a.lower() in emotiList])\n",
    "            kwonfeatures[topic][category][tweet[\"id\"]][\"abbreviationCount\"] = len([a for a in tweet[\"full_text\"].split(\" \") if a.lower() in abbrList])\n",
    "            kwonfeatures[topic][category][tweet[\"id\"]][\"emojiCount\"] = len([x for x in tweet[\"full_text\"].split(\" \") if x in emoji.EMOJI_UNICODE.keys() or x in emoji.EMOJI_UNICODE.values()])\n",
    " \n",
    "            kwonfeatures[topic][category][tweet[\"id\"]][\"posAdjectiveCount\"] = len([x for x in spacyTweetText if x.pos_ == \"ADJ\"])\n",
    "            kwonfeatures[topic][category][tweet[\"id\"]][\"posAdpositionCount\"] = len([x for x in spacyTweetText if x.pos_ == \"ADP\"])\n",
    "            kwonfeatures[topic][category][tweet[\"id\"]][\"posAdverbCount\"] = len([x for x in spacyTweetText if x.pos_ == \"ADV\"])\n",
    "            kwonfeatures[topic][category][tweet[\"id\"]][\"posAuxiliaryCount\"] = len([x for x in spacyTweetText if x.pos_ == \"AUX\"])\n",
    "            kwonfeatures[topic][category][tweet[\"id\"]][\"posConjunctionCount\"] = len([x for x in spacyTweetText if x.pos_ == \"CONJ\"])\n",
    "            kwonfeatures[topic][category][tweet[\"id\"]][\"posCoordinatingConjunctionCount\"] = len([x for x in spacyTweetText if x.pos_ == \"CCONJ\"])\n",
    "            kwonfeatures[topic][category][tweet[\"id\"]][\"posDeterminerCount\"] = len([x for x in spacyTweetText if x.pos_ == \"DET\"])\n",
    "            kwonfeatures[topic][category][tweet[\"id\"]][\"posInterjectionCount\"] = len([x for x in spacyTweetText if x.pos_ == \"INTJ\"])\n",
    "            kwonfeatures[topic][category][tweet[\"id\"]][\"posNounCount\"] = len([x for x in spacyTweetText if x.pos_ == \"NOUN\"])\n",
    "            kwonfeatures[topic][category][tweet[\"id\"]][\"posNumeralCount\"] = len([x for x in spacyTweetText if x.pos_ == \"NUM\"])\n",
    "            kwonfeatures[topic][category][tweet[\"id\"]][\"posParticleCount\"] = len([x for x in spacyTweetText if x.pos_ == \"PART\"])\n",
    "            kwonfeatures[topic][category][tweet[\"id\"]][\"posPronounCount\"] = len([x for x in spacyTweetText if x.pos_ == \"PRON\"])\n",
    "            kwonfeatures[topic][category][tweet[\"id\"]][\"posProperNounCount\"] = len([x for x in spacyTweetText if x.pos_ == \"PROPN\"])\n",
    "            kwonfeatures[topic][category][tweet[\"id\"]][\"posPunctuationCount\"] = len([x for x in spacyTweetText if x.pos_ == \"PUNCT\"])\n",
    "            kwonfeatures[topic][category][tweet[\"id\"]][\"posSubordinatingConjunctionCount\"] = len([x for x in spacyTweetText if x.pos_ == \"SCONJ\"])\n",
    "            kwonfeatures[topic][category][tweet[\"id\"]][\"posSymbolCount\"] = len([x for x in spacyTweetText if x.pos_ == \"SYM\"])\n",
    "            kwonfeatures[topic][category][tweet[\"id\"]][\"posVerbCount\"] = len([x for x in spacyTweetText if x.pos_ == \"VERB\"])\n",
    "            kwonfeatures[topic][category][tweet[\"id\"]][\"posOtherCount\"] = len([x for x in spacyTweetText if x.pos_ == \"X\"])\n",
    "            kwonfeatures[topic][category][tweet[\"id\"]][\"posSpaceCount\"] = len([x for x in spacyTweetText if x.pos_ == \"SPACE\"])\n",
    "            kwonfeatures[topic][category][tweet[\"id\"]][\"nerPersonCount\"] = len([x for x in spacyTweetText if x.ent_type_ == \"Person\"])\n",
    "            kwonfeatures[topic][category][tweet[\"id\"]][\"nerNationalityCount\"] = len([x for x in spacyTweetText if x.ent_type_ == \"NORP\"])\n",
    "            kwonfeatures[topic][category][tweet[\"id\"]][\"nerBuildingCount\"] = len([x for x in spacyTweetText if x.ent_type_ == \"FAC\"])\n",
    "            kwonfeatures[topic][category][tweet[\"id\"]][\"nerOrganizationCount\"] = len([x for x in spacyTweetText if x.ent_type_ == \"ORG\"])\n",
    "            kwonfeatures[topic][category][tweet[\"id\"]][\"nerCountriesCount\"] = len([x for x in spacyTweetText if x.ent_type_ == \"GPE\"])\n",
    "            kwonfeatures[topic][category][tweet[\"id\"]][\"nerLocationCount\"] = len([x for x in spacyTweetText if x.ent_type_ == \"LOC\"])\n",
    "            kwonfeatures[topic][category][tweet[\"id\"]][\"nerProductCount\"] = len([x for x in spacyTweetText if x.ent_type_ == \"PRODUCT\"])\n",
    "            kwonfeatures[topic][category][tweet[\"id\"]][\"nerEventCount\"] = len([x for x in spacyTweetText if x.ent_type_ == \"EVENT\"])\n",
    "            kwonfeatures[topic][category][tweet[\"id\"]][\"nerArtCount\"] = len([x for x in spacyTweetText if x.ent_type_ == \"WORK_OF_ART\"])\n",
    "            kwonfeatures[topic][category][tweet[\"id\"]][\"nerLawCount\"] = len([x for x in spacyTweetText if x.ent_type_ == \"LAW\"])\n",
    "            kwonfeatures[topic][category][tweet[\"id\"]][\"nerLanguageCount\"] = len([x for x in spacyTweetText if x.ent_type_ == \"LANGUAGE\"])\n",
    "            kwonfeatures[topic][category][tweet[\"id\"]][\"nerDateCount\"] = len([x for x in spacyTweetText if x.ent_type_ == \"DATE\"])\n",
    "            kwonfeatures[topic][category][tweet[\"id\"]][\"nerTimeCount\"] = len([x for x in spacyTweetText if x.ent_type_ == \"TIME\"])\n",
    "            kwonfeatures[topic][category][tweet[\"id\"]][\"nerMoneyCount\"] = len([x for x in spacyTweetText if x.ent_type_ == \"MONEY\"])\n",
    "            kwonfeatures[topic][category][tweet[\"id\"]][\"nerQuantityCount\"] = len([x for x in spacyTweetText if x.ent_type_ == \"QUANTITY\"])\n",
    "            kwonfeatures[topic][category][tweet[\"id\"]][\"nerOrdinalCount\"] = len([x for x in spacyTweetText if x.ent_type_ == \"ORDINAL\"])\n",
    "            kwonfeatures[topic][category][tweet[\"id\"]][\"nerCardinalCount\"] = len([x for x in spacyTweetText if x.ent_type_ == \"Cardinal\"])\n",
    "\n",
    "            kwonfeatures[topic][category][tweet[\"id\"]][\"insight\"] = kLIWC[topic][category][str(tweet[\"id\"])][\"insight\"]\n",
    "            kwonfeatures[topic][category][tweet[\"id\"]][\"tentative\"] = kLIWC[topic][category][str(tweet[\"id\"])][\"tentat\"]\n",
    "            kwonfeatures[topic][category][tweet[\"id\"]][\"positiveEmotion\"] = kLIWC[topic][category][str(tweet[\"id\"])][\"posemo\"]\n",
    "            kwonfeatures[topic][category][tweet[\"id\"]][\"negativeEmotion\"] = kLIWC[topic][category][str(tweet[\"id\"])][\"negemo\"]\n",
    "            kwonfeatures[topic][category][tweet[\"id\"]][\"anxiety\"] = kLIWC[topic][category][str(tweet[\"id\"])][\"anx\"]\n",
    "            kwonfeatures[topic][category][tweet[\"id\"]][\"certainty\"] = kLIWC[topic][category][str(tweet[\"id\"])][\"certain\"]\n",
    "            kwonfeatures[topic][category][tweet[\"id\"]][\"tone\"] = kLIWC[topic][category][str(tweet[\"id\"])][\"Tone\"]\n",
    "            try:\n",
    "                parser=nlp2.parse(tweet[\"full_text\"]) \n",
    "                tree=Tree.fromstring(parser.__str__()) \n",
    "                kwonfeatures[topic][category][tweet[\"id\"]][\"sentenceComplexity\"] = tree.height() \n",
    "            except:\n",
    "                excepts.append(tweet)\n",
    "            \n",
    "# User features\n",
    "\n",
    "            kwonfeatures[topic][category][tweet[\"id\"]][\"hasProfileDescription\"] = True if tweet[\"user\"][\"description\"] != '' else False\n",
    "            kwonfeatures[topic][category][tweet[\"id\"]][\"isVerifiedAccount\"] = tweet[\"user\"][\"verified\"] \n",
    "            kwonfeatures[topic][category][tweet[\"id\"]][\"statusCount\"] = tweet[\"user\"][\"statuses_count\"]\n",
    "            kwonfeatures[topic][category][tweet[\"id\"]][\"followingCount\"] = tweet[\"user\"][\"friends_count\"]\n",
    "            kwonfeatures[topic][category][tweet[\"id\"]][\"influnece\"] = tweet[\"user\"][\"followers_count\"]\n",
    "            try:\n",
    "                kwonfeatures[topic][category][tweet[\"id\"]][\"userRole\"] = tweet[\"user\"][\"followers_count\"] / tweet[\"user\"][\"friends_count\"] \n",
    "            except:\n",
    "                kwonfeatures[topic][category][tweet[\"id\"]][\"userRole\"] = float(\"inf\")\n",
    "                excepts.append(tweet)\n",
    "            kwonfeatures[topic][category][tweet[\"id\"]][\"totalProfileLikesCount\"] = tweet[\"user\"][\"favourites_count\"]\n",
    "            accountCreationTime = datetime.datetime.strptime(tweet[\"user\"][\"created_at\"], \"%a %b %d %H:%M:%S %z %Y\")\n",
    "            today = datetime.datetime.now()\n",
    "            kwonfeatures[topic][category][tweet[\"id\"]][\"accountAge\"] = (today.date() - accountCreationTime.date()).days\n",
    "            kwonfeatures[topic][category][tweet[\"id\"]][\"protectedProfile\"] = tweet[\"user\"][\"protected\"]\n",
    "            kwonfeatures[topic][category][tweet[\"id\"]][\"hasProfileLocation\"] = True if tweet[\"user\"][\"location\"] != '' else False\n",
    "            kwonfeatures[topic][category][tweet[\"id\"]][\"profileLocation\"] = tweet[\"user\"][\"location\"]\n",
    "            kwonfeatures[topic][category][tweet[\"id\"]][\"hasProfilePicture\"] = True if tweet[\"user\"][\"profile_image_url\"] != '' else False\n",
    "            kwonfeatures[topic][category][tweet[\"id\"]][\"geoEnabled\"] = tweet[\"user\"][\"geo_enabled\"]\n",
    "            kwonfeatures[topic][category][tweet[\"id\"]][\"hasProfileUrl\"] = tweet[\"user\"][\"url\"]\n",
    "            kwonfeatures[topic][category][tweet[\"id\"]][\"averageFollowSpeed\"] = tweet[\"user\"][\"followers_count\"] / kwonfeatures[topic][category][tweet[\"id\"]][\"accountAge\"]\n",
    "            kwonfeatures[topic][category][tweet[\"id\"]][\"averageBeingFollowedSpeed\"] = tweet[\"user\"][\"friends_count\"] / kwonfeatures[topic][category][tweet[\"id\"]][\"accountAge\"]\n",
    "            kwonfeatures[topic][category][tweet[\"id\"]][\"averageLikeSpeed\"] = tweet[\"user\"][\"favourites_count\"] / kwonfeatures[topic][category][tweet[\"id\"]][\"accountAge\"]\n",
    "            kwonfeatures[topic][category][tweet[\"id\"]][\"averageStatusSpeed\"] = tweet[\"user\"][\"statuses_count\"] / kwonfeatures[topic][category][tweet[\"id\"]][\"accountAge\"]\n",
    "            kwonfeatures[topic][category][tweet[\"id\"]][\"screenNameLength\"] = len(tweet[\"user\"][\"screen_name\"])\n",
    "            kwonfeatures[topic][category][tweet[\"id\"]][\"screenNameDigitCount\"] = len([i for i in tweet[\"user\"][\"screen_name\"] if i in [str(k) for k in range(0,10)]])\n",
    "            \n",
    "# Meta message features\n",
    "        \n",
    "            kwonfeatures[topic][category][tweet[\"id\"]][\"hashtagCount\"] = len(tweet[\"entities\"][\"hashtags\"])\n",
    "            kwonfeatures[topic][category][tweet[\"id\"]][\"mentionCount\"] = len(tweet[\"entities\"][\"user_mentions\"])\n",
    "            kwonfeatures[topic][category][tweet[\"id\"]][\"hasUrl\"] = (\"http://\" in tweet[\"full_text\"] or \"https://\" in tweet[\"full_text\"] or len(tweet[\"entities\"][\"urls\"]) > 0)\n",
    "            kwonfeatures[topic][category][tweet[\"id\"]][\"multimediaCounter\"] = len(tweet[\"entities\"][\"media\"]) if \"media\" in tweet[\"entities\"].keys() else 0\n",
    "            kwonfeatures[topic][category][tweet[\"id\"]][\"likeCount\"] = tweet[\"favorite_count\"]\n",
    "            kwonfeatures[topic][category][tweet[\"id\"]][\"retweetCount\"] = tweet[\"retweet_count\"]\n",
    "            tweetCreationTime = datetime.datetime.strptime(tweet[\"created_at\"], \"%a %b %d %H:%M:%S %z %Y\")\n",
    "            kwonfeatures[topic][category][tweet[\"id\"]][\"tweetPostTime\"] = tweetCreationTime.hour*3600 + tweetCreationTime.minute*60 + tweetCreationTime.second \n",
    "            kwonfeatures[topic][category][tweet[\"id\"]][\"place\"] = tweet[\"place\"]\n",
    "\n",
    "#Compound score\n",
    "\n",
    "            kwonfeatures[topic][category][tweet[\"id\"]][\"credibilityScore\"] = tweet[\"user\"][\"verified\"] \\\n",
    "                                                                                * (kwonfeatures[topic][category][tweet[\"id\"]][\"accountAge\"] + tweet[\"user\"][\"followers_count\"] \\\n",
    "                                                                                   + tweet[\"user\"][\"statuses_count\"] + tweet[\"favorite_count\"]) \n",
    "            kwonfeatures[topic][category][tweet[\"id\"]][\"engagementScore\"] = (tweet[\"user\"][\"statuses_count\"] + tweet[\"favorite_count\"])/kwonfeatures[topic][category][tweet[\"id\"]][\"accountAge\"]\n",
    "\n",
    "#Other features\n",
    "            \n",
    "            pattern = re.compile('[>].*[<]')\n",
    "            try:\n",
    "                kwonfeatures[topic][category][tweet[\"id\"]][\"source\"] = pattern.findall(tweet[\"source\"])[0][1:-1]\n",
    "            except:\n",
    "                kwonfeatures[topic][category][tweet[\"id\"]][\"source\"] = tweet[\"source\"]\n",
    "                excepts.append(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zubiagafeaturesForDataframe = {}\n",
    "for topic in tqdm(zubiagafeatures):\n",
    "    for category in zubiagafeatures[topic]:\n",
    "        for tweet_id in zubiagafeatures[topic][category]:\n",
    "            zubiagafeaturesForDataframe[tweet_id] = zubiagafeatures[topic][category][tweet_id]\n",
    "kwonfeaturesForDataframe = {}\n",
    "for topic in tqdm(kwonfeatures):\n",
    "    for category in kwonfeatures[topic]:\n",
    "        for tweet_id in kwonfeatures[topic][category]:\n",
    "            kwonfeaturesForDataframe[tweet_id] = kwonfeatures[topic][category][tweet_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zubiagaFeatures_raw = pd.DataFrame.from_dict(zubiagafeaturesForDataframe, orient=\"index\")\n",
    "df_kwonFeatures_raw = pd.DataFrame.from_dict(kwonfeaturesForDataframe, orient=\"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stroing the dataframe on the hard drive\n",
    "pk.dump(df_zubiagaFeatures_raw, open(\"./df_zubiagaFeatures_raw\",\"wb\"))\n",
    "pk.dump(df_kwonFeatures_raw, open(\"./df_kwonFeatures_raw\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leaving out the non-numerical features and non-early features\n",
    "df_zubiagaFeatures_processed_v1 = df_zubiagaFeatures_raw.drop(columns=['topic', 'id', 'text', 'screenName', 'tweetUrl', 'source', 'hasProfileUrl', 'likeCount', 'retweetCount'])\n",
    "df_kwonFeatures_processed_v1 = df_kwonFeatures_raw.drop(columns=['topic', 'id', 'text', 'screenName', 'tweetUrl', 'source', 'hasProfileUrl'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turning all bollean columns to integer\n",
    "df_zubiagaFeatures_processed_v2 = copy.deepcopy(df_zubiagaFeatures_processed_v1)\n",
    "df_kwonFeatures_processed_v2 = copy.deepcopy(df_kwonFeatures_processed_v1)\n",
    "boolCol = [i for i in df_zubiagaFeatures_processed_v1.columns if df_zubiagaFeatures_processed_v1.dtypes[i] == 'bool']\n",
    "for col in boolCol:\n",
    "    df_zubiagaFeatures_processed_v2[col] = df_zubiagaFeatures_processed_v1[col].astype(int)\n",
    "    df_kwonFeatures_processed_v2[col] = df_kwonFeatures_processed_v1[col].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Turning all commas to dots!\n",
    "for col in df_zubiagaFeatures_processed_v2.columns:\n",
    "    df_zubiagaFeatures_processed_v2[col] = df_zubiagaFeatures_processed_v2[col].replace(\",\",\".\",regex=True) \n",
    "    df_kwonFeatures_processed_v2[col] = df_kwonFeatures_processed_v2[col].replace(\",\",\".\",regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Turning some innocent object data to numeric version!\n",
    "for col in ['insight', 'tentative', 'positiveEmotion', 'negativeEmotion', 'anxiety', 'certainty', 'tone']:\n",
    "    df_zubiagaFeatures_processed_v2[col] = df_zubiagaFeatures_processed_v2[col].astype('float64')\n",
    "    df_kwonFeatures_processed_v2[col] = df_kwonFeatures_processed_v2[col].astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminating all the object data types\n",
    "objectCol = [i for i in df_zubiagaFeatures_processed_v2.columns if df_zubiagaFeatures_processed_v2.dtypes[i] == 'object' and i != 'category']\n",
    "for col in objectCol:\n",
    "    df_zubiagaFeatures_processed_v2 = df_zubiagaFeatures_processed_v2.drop(col,axis=\"columns\")\n",
    "    df_kwonFeatures_processed_v2 = df_kwonFeatures_processed_v2.drop(col,axis=\"columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check for infinite values\n",
    "df_zubiagaFeatures_processed_v2 = df_zubiagaFeatures_processed_v2.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "df_kwonFeatures_processed_v2 = df_kwonFeatures_processed_v2.replace([np.inf, -np.inf], np.nan).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check to see whether there is any null value\n",
    "for col in df_zubiagaFeatures_processed_v2.columns:\n",
    "    print(f'{col}: {pd.isnull(df_zubiagaFeatures_processed_v2[col]).any()}')\n",
    "print(\"------------\")\n",
    "for col in df_kwonFeatures_processed_v2.columns:\n",
    "    print(f'{col}: {pd.isnull(df_kwonFeatures_processed_v2[col]).any()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More sanity check about null/missing values in the data frame\n",
    "for col in df_zubiagaFeatures_processed_v2:\n",
    "    print(df_zubiagaFeatures_processed_v2[col].isnull().any())\n",
    "print(\"-----------\")\n",
    "for col in df_kwonFeatures_processed_v2:\n",
    "    print(df_kwonFeatures_processed_v2[col].isnull().any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More sanity check about null/missing values in the data frame\n",
    "for col in df_zubiagaFeatures_processed_v2:\n",
    "    print(df_zubiagaFeatures_processed_v2[col].isnull().values.sum())\n",
    "print(\"-----------\")\n",
    "for col in df_kwonFeatures_processed_v2:\n",
    "    print(df_kwonFeatures_processed_v2[col].isnull().values.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More sanity check about null/missing values in the data frame\n",
    "print(df_zubiagaFeatures_processed_v2[col].isnull().values.sum().sum())\n",
    "print(\"-----------\")\n",
    "print(df_kwonFeatures_processed_v2[col].isnull().values.sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separating rumour from nonRumour\n",
    "df_zubiagaFeatures_processed_v2_rumour = df_zubiagaFeatures_processed_v2.drop(df_zubiagaFeatures_processed_v2[df_zubiagaFeatures_processed_v2.category != \"rumour\"].index)\n",
    "df_zubiagaFeatures_processed_v2_rumour.drop([\"category\"], inplace=True, axis=\"columns\")\n",
    "df_zubiagaFeatures_processed_v2_nonRumour = df_zubiagaFeatures_processed_v2.drop(df_zubiagaFeatures_processed_v2[df_zubiagaFeatures_processed_v2.category != \"nonRumour\"].index)\n",
    "df_zubiagaFeatures_processed_v2_nonRumour.drop([\"category\"], inplace=True, axis=\"columns\")\n",
    "\n",
    "df_kwonFeatures_processed_v2_rumour = df_kwonFeatures_processed_v2.drop(df_kwonFeatures_processed_v2[df_kwonFeatures_processed_v2.category != \"rumour\"].index)\n",
    "df_kwonFeatures_processed_v2_rumour.drop([\"category\"], inplace=True, axis=\"columns\")\n",
    "df_kwonFeatures_processed_v2_nonRumour = df_kwonFeatures_processed_v2.drop(df_kwonFeatures_processed_v2[df_kwonFeatures_processed_v2.category != \"nonRumour\"].index)\n",
    "df_kwonFeatures_processed_v2_nonRumour.drop([\"category\"], inplace=True, axis=\"columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More sanity check about null/missing values in the data frame\n",
    "\n",
    "print(df_zubiagaFeatures_processed_v2_rumour.isnull().any(None))\n",
    "print(df_zubiagaFeatures_processed_v2_nonRumour.isnull().any(None))\n",
    "print(df_kwonFeatures_processed_v2_rumour.isnull().any(None))\n",
    "print(df_kwonFeatures_processed_v2_nonRumour.isnull().any(None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_zubiagaFeatures_processed_v2_rumour\n",
    "# df_zubiagaFeatures_processed_v2_nonRumour\n",
    "# df_kwonFeatures_processed_v2_rumour\n",
    "# df_kwonFeatures_processed_v2_nonRumour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalization\n",
    "x1 = df_zubiagaFeatures_processed_v2_rumour.values \n",
    "min_max_scaler1 = preprocessing.MinMaxScaler()\n",
    "x_scaled1 = min_max_scaler1.fit_transform(x1)\n",
    "df_zubiagaFeatures_processed_v2_rumour_normalized=pd.DataFrame(x_scaled1, columns=df_zubiagaFeatures_processed_v2_rumour.columns)\n",
    "\n",
    "x2 = df_zubiagaFeatures_processed_v2_nonRumour.values \n",
    "min_max_scaler2 = preprocessing.MinMaxScaler()\n",
    "x_scaled2 = min_max_scaler2.fit_transform(x2)\n",
    "df_zubiagaFeatures_processed_v2_nonRumour_normalized=pd.DataFrame(x_scaled2, columns=df_zubiagaFeatures_processed_v2_nonRumour.columns)\n",
    "\n",
    "x3 = df_kwonFeatures_processed_v2_rumour.values \n",
    "min_max_scaler3 = preprocessing.MinMaxScaler()\n",
    "x_scaled3 = min_max_scaler3.fit_transform(x3)\n",
    "df_kwonFeatures_processed_v2_rumour_normalized=pd.DataFrame(x_scaled3, columns=df_kwonFeatures_processed_v2_rumour.columns)\n",
    "\n",
    "x4 = df_kwonFeatures_processed_v2_nonRumour.values \n",
    "min_max_scaler4 = preprocessing.MinMaxScaler()\n",
    "x_scaled4 = min_max_scaler4.fit_transform(x4)\n",
    "df_kwonFeatures_processed_v2_nonRumour_normalized=pd.DataFrame(x_scaled4, columns=df_kwonFeatures_processed_v2_nonRumour.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zubiagaFeatures_processed_v2_rumour_normalized.to_csv(\"./zubiagaFeatures_rumours.csv\")\n",
    "df_zubiagaFeatures_processed_v2_nonRumour_normalized.to_csv(\"./zubiagaFeatures_nonRumours.csv\")\n",
    "df_kwonFeatures_processed_v2_rumour_normalized.to_csv(\"./kwonFeatures_rumours.csv\")\n",
    "df_kwonFeatures_processed_v2_nonRumour_normalized.to_csv(\"./kwonFeatures_nonRumours.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
